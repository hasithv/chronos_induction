{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hasith/miniconda3/envs/chronos_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-09 12:59:20,046] [WARNING] [real_accelerator.py:194:get_accelerator] Setting accelerator to CPU. If you have GPU or other accelerator, we were unable to detect it.\n",
      "[2025-05-09 12:59:20,048] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cpu (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  # requires: pip install pandas\n",
    "import torch\n",
    "from chronos import BaseChronosPipeline\n",
    "import plotly.express as px\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=512, out_features=512, bias=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = BaseChronosPipeline.from_pretrained(\n",
    "    \"amazon/chronos-t5-small\",  # use \"amazon/chronos-bolt-small\" for the corresponding Chronos-Bolt model\n",
    "    device_map=\"cpu\",  # use \"cpu\" for CPU inference\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "layer = pipeline.model\n",
    "layer_name = 'model.decoder.block.0.layer.0'\n",
    "for name in layer_name.split('.'):\n",
    "    # print(\"processing: \", name)\n",
    "    layer = getattr(layer, name)\n",
    "layer.SelfAttention.q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=512, out_features=512, bias=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.model.get_submodule('model.decoder.block.0.layer.0.SelfAttention.q')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.model.model.decoder.block[0].layer[0].SelfAttention.n_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5Attention(\n",
       "  (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "  (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "  (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "  (o): Linear(in_features=512, out_features=512, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.model.get_submodule('model.decoder.block.0.layer.1.EncDecAttention')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get activations of layer 'model.encoder.embed_tokens' for the data frame input's token_ids\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/AileenNielsen/TimeSeriesAnalysisWithPython/master/data/AirPassengers.csv\")\n",
    "context = torch.tensor(df[\"#Passengers\"])\n",
    "context_tensor = pipeline._prepare_and_validate_context(context=context) # does nothing\n",
    "token_ids, attention_mask, scale = pipeline.tokenizer.context_input_transform(context_tensor)\n",
    "\n",
    "model = pipeline.model\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get activations of layers for some input on a model\n",
    "def get_output_activations(layers, input, model, attention_mask=attention_mask, prediction_length=1):\n",
    "    activations = []\n",
    "    handles = []\n",
    "\n",
    "    def hook_fn(module, input, output):\n",
    "        activations.append(output)\n",
    "\n",
    "    for layer in layers:\n",
    "        handle = layer.register_forward_hook(hook_fn)\n",
    "        handles.append(handle)\n",
    "    outputs = model(input_ids=input, attention_mask=attention_mask, prediction_length=4)\n",
    "    for handle in handles:\n",
    "        handle.remove()\n",
    "    \n",
    "    return outputs, activations\n",
    "\n",
    "def get_input_activations(layers, input, model, attention_mask=attention_mask, prediction_length=1):\n",
    "    activations = []\n",
    "    handles = []\n",
    "\n",
    "    def hook_fn(module, input, output):\n",
    "        activations.append(input)\n",
    "\n",
    "    for layer in layers:\n",
    "        handle = layer.register_forward_hook(hook_fn)\n",
    "        handles.append(handle)\n",
    "\n",
    "    # Run the model\n",
    "    outputs = model(input_ids=input, attention_mask=attention_mask, prediction_length=prediction_length)\n",
    "    for handle in handles:\n",
    "        handle.remove()\n",
    "    return outputs, activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get activations of layer 'model.encoder.embed_tokens' for the data frame input's token_ids\n",
    "# outputs,activations = get_output_activations([model.model.encoder.embed_tokens], token_ids, model, attention_mask=attention_mask, prediction_length=1)\n",
    "\n",
    "# get input activations of layer 'model.decoder.block' for the data frame input's token_ids\n",
    "outputs, activations = get_input_activations([pipeline.model.get_submodule('model.decoder.block.0.layer.0.SelfAttention.q')], token_ids, model, attention_mask=attention_mask, prediction_length=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 145])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder‑input shapes at each step:\n",
      "  step  0: (tensor([[2104, 2106, 2113, 2112, 2108, 2115, 2121, 2121, 2115, 2107, 2100, 2106,\n",
      "         2105, 2110, 2118, 2115, 2110, 2122, 2132, 2132, 2126, 2114, 2104, 2117,\n",
      "         2120, 2122, 2136, 2128, 2133, 2136, 2146, 2146, 2139, 2128, 2120, 2130,\n",
      "         2132, 2137, 2143, 2137, 2138, 2155, 2161, 2167, 2151, 2142, 2133, 2143,\n",
      "         2144, 2144, 2164, 2163, 2160, 2167, 2177, 2181, 2164, 2152, 2137, 2147,\n",
      "         2148, 2140, 2163, 2159, 2163, 2177, 2196, 2192, 2175, 2160, 2148, 2160,\n",
      "         2167, 2162, 2179, 2180, 2180, 2202, 2226, 2218, 2201, 2182, 2164, 2184,\n",
      "         2187, 2184, 2203, 2201, 2204, 2231, 2250, 2246, 2222, 2198, 2181, 2198,\n",
      "         2202, 2195, 2222, 2218, 2222, 2254, 2275, 2276, 2246, 2218, 2197, 2213,\n",
      "         2214, 2204, 2225, 2218, 2226, 2261, 2288, 2295, 2246, 2224, 2200, 2213,\n",
      "         2224, 2215, 2247, 2242, 2253, 2279, 2316, 2321, 2274, 2247, 2225, 2246,\n",
      "         2252, 2239, 2253, 2273, 2279, 2309, 2352, 2344, 2296, 2273, 2239, 2259,\n",
      "            1]]),)\n",
      "  step  1: (tensor([[0]]),)\n",
      "  step  2: (tensor([[   0, 2262]]),)\n",
      "  step  3: (tensor([[   0, 2262, 2249]]),)\n",
      "  step  4: (tensor([[   0, 2262, 2249, 2267]]),)\n",
      "  step  5: (tensor([[   0, 2262, 2249, 2267, 2277]]),)\n"
     ]
    }
   ],
   "source": [
    "q_layer = model.get_submodule('model.decoder.block.0.layer.0.SelfAttention.q')\n",
    "\n",
    "# 3) Prepare a place to record each decoder-input shape\n",
    "shapes = []\n",
    "def pre_hook(module, inputs):\n",
    "    # inputs[0] is the tensor of token IDs fed into embed_tokens\n",
    "    shapes.append(inputs)\n",
    "\n",
    "# Register the hook on the decoder’s embed_tokens\n",
    "# handle = q_layer.register_forward_pre_hook(pre_hook)\n",
    "handle = model.model.decoder.embed_tokens.register_forward_pre_hook(pre_hook)\n",
    "\n",
    "# 4) Generate with caching **disabled** and just **1** sequence path\n",
    "#    (so we see the full prefix each time)\n",
    "_ = model.model.generate(\n",
    "    input_ids=token_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_new_tokens=5,           # e.g. generate 5 steps\n",
    "    num_return_sequences=1,     # just one path\n",
    "    do_sample=False,            # deterministic for clarity\n",
    "    use_cache=False,            # IMPORTANT: force full-prefix decoding\n",
    ")\n",
    "\n",
    "# 5) Remove the hook\n",
    "handle.remove()\n",
    "\n",
    "# 6) Inspect what we saw\n",
    "print(\"Decoder‑input shapes at each step:\")\n",
    "for step, shape in enumerate(shapes):\n",
    "    print(f\"  step {step:>2}: {shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 145, 512])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs = model.model.encoder(token_ids, attention_mask)\n",
    "encoder_outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input_ids = torch.zeros(size=(20,1), dtype=torch.int)\n",
    "enc_h = encoder_outputs.last_hidden_state.expand(20, -1, -1).contiguous()\n",
    "decoder_outputs = model.model.decoder(input_ids=decoder_input_ids, encoder_hidden_states=enc_h,\n",
    "                                      encoder_attention_mask=attention_mask, past_key_values=None, use_cache=False, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ -80.5000,  -16.7500, -246.0000,  ..., -184.0000, -200.0000,\n",
       "           45.7500], dtype=torch.bfloat16, grad_fn=<SelectBackward0>),\n",
       " tensor([ -80.5000,  -16.7500, -246.0000,  ..., -184.0000, -200.0000,\n",
       "           45.7500], dtype=torch.bfloat16, grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_logits = model.model.lm_head(decoder_outputs.last_hidden_state)\n",
    "# get the logits for the next token\n",
    "next_token_logits = lm_logits[:, -1, :]\n",
    "next_token_logits[0], next_token_logits[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 1, 4096])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When the model is given some input, write a function which will return the input token_ids that have been fed into the embedding layer\n",
    "def get_input_token_ids(model, input, attention_mask):\n",
    "    activations = []\n",
    "    handles = []\n",
    "\n",
    "    def hook_fn(module, input, output):\n",
    "        activations.append(input)\n",
    "\n",
    "    handle = model.model.shared.register_forward_hook(hook_fn)\n",
    "    handles.append(handle)\n",
    "\n",
    "    outputs = model(input_ids=input, attention_mask=attention_mask, prediction_length=4)\n",
    "    for handle in handles:\n",
    "        handle.remove()\n",
    "    \n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([[2104, 2106, 2113, 2112, 2108, 2115, 2121, 2121, 2115, 2107, 2100, 2106,\n",
       "           2105, 2110, 2118, 2115, 2110, 2122, 2132, 2132, 2126, 2114, 2104, 2117,\n",
       "           2120, 2122, 2136, 2128, 2133, 2136, 2146, 2146, 2139, 2128, 2120, 2130,\n",
       "           2132, 2137, 2143, 2137, 2138, 2155, 2161, 2167, 2151, 2142, 2133, 2143,\n",
       "           2144, 2144, 2164, 2163, 2160, 2167, 2177, 2181, 2164, 2152, 2137, 2147,\n",
       "           2148, 2140, 2163, 2159, 2163, 2177, 2196, 2192, 2175, 2160, 2148, 2160,\n",
       "           2167, 2162, 2179, 2180, 2180, 2202, 2226, 2218, 2201, 2182, 2164, 2184,\n",
       "           2187, 2184, 2203, 2201, 2204, 2231, 2250, 2246, 2222, 2198, 2181, 2198,\n",
       "           2202, 2195, 2222, 2218, 2222, 2254, 2275, 2276, 2246, 2218, 2197, 2213,\n",
       "           2214, 2204, 2225, 2218, 2226, 2261, 2288, 2295, 2246, 2224, 2200, 2213,\n",
       "           2224, 2215, 2247, 2242, 2253, 2279, 2316, 2321, 2274, 2247, 2225, 2246,\n",
       "           2252, 2239, 2253, 2273, 2279, 2309, 2352, 2344, 2296, 2273, 2239, 2259,\n",
       "              1]]),),\n",
       " (tensor([[0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0]]),),\n",
       " (tensor([[2262],\n",
       "          [2268],\n",
       "          [2259],\n",
       "          [2266],\n",
       "          [2260],\n",
       "          [2267],\n",
       "          [2261],\n",
       "          [2259],\n",
       "          [2273],\n",
       "          [2257],\n",
       "          [2261],\n",
       "          [2266],\n",
       "          [2265],\n",
       "          [2259],\n",
       "          [2257],\n",
       "          [2266],\n",
       "          [2266],\n",
       "          [2259],\n",
       "          [2262],\n",
       "          [2258]]),),\n",
       " (tensor([[2257],\n",
       "          [2251],\n",
       "          [2243],\n",
       "          [2258],\n",
       "          [2245],\n",
       "          [2253],\n",
       "          [2250],\n",
       "          [2240],\n",
       "          [2264],\n",
       "          [2236],\n",
       "          [2259],\n",
       "          [2252],\n",
       "          [2249],\n",
       "          [2246],\n",
       "          [2248],\n",
       "          [2257],\n",
       "          [2252],\n",
       "          [2246],\n",
       "          [2249],\n",
       "          [2240]]),),\n",
       " (tensor([[2274],\n",
       "          [2265],\n",
       "          [2267],\n",
       "          [2274],\n",
       "          [2267],\n",
       "          [2266],\n",
       "          [2275],\n",
       "          [2257],\n",
       "          [2285],\n",
       "          [2250],\n",
       "          [2277],\n",
       "          [2269],\n",
       "          [2272],\n",
       "          [2258],\n",
       "          [2272],\n",
       "          [2270],\n",
       "          [2273],\n",
       "          [2261],\n",
       "          [2268],\n",
       "          [2250]]),)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_input_token_ids(model, token_ids, attention_mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chronos_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
